---
title: "K Nearest Neighbors"
author: "R.M. Palmer"
date: "11/18/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r child = 'dataload.Rmd'}
```

```{r}
library(caret)
library(class)
```

```{r prep_for_knn}
# scale all numeric variables to normalize
# then select only the variable to be predicted and the normalized values
for_knn <- employee_data %>%
  select(Attrition,
         z_DistanceFromHome,
         z_Education,
         z_EnvironmentSatisfaction,
         z_JobInvolvement,
         z_JobLevel,
         z_JobSatisfaction,
         z_NumCompaniesWorked,
         z_PercentSalaryHike,
         z_PerformanceRating,
         z_RelationshipSatisfaction,
         z_StockOptionLevel,
         z_TotalWorkingYears,
         z_TrainingTimesLastYear,
         z_WorkLifeBalance,
         z_YearsAtCompany,
         z_YearsInCurrentRole,
         z_YearsSinceLastPromotion,
         z_YearsWithCurrManager)

# selected by stepwise AIC
cols_for_knn <- c("z_DistanceFromHome",
         "z_EnvironmentSatisfaction",
         "z_JobInvolvement",
         "z_JobSatisfaction",
         "z_NumCompaniesWorked",
         "z_RelationshipSatisfaction",
         "z_StockOptionLevel",
         "z_TotalWorkingYears",
         "z_TrainingTimesLastYear",
         "z_WorkLifeBalance",
         "z_YearsInCurrentRole",
         "z_YearsSinceLastPromotion")

# omit because anova says not important
# z_YearsSinceLastPromotion
# education
# trainingTimesLastYear
# RelationshipSatisfaction
# PerformanceRating
# PercentSalaryHike
# NumCompaniesWorked
# ...
         
# adjust the knn results by known probabilities
knn_adjust <- function(orig_factor, num_no=1, num_yes=1)
{
  # adjustment factors
  n_factor <- 1/num_no
  y_factor <- 1/num_yes
  
  # original probabilities
  orig_win    <- attr(orig_factor,'prob')
  orig_lose   <- 1.0 - orig_win
  
  # What was original answer
  orig_answer <- as.character(orig_factor)
  other_answer <- ifelse(orig_answer == 'No','Yes','No')
  
  # if the original answer was no, then adjust original probability by the no factor
  new_win     <- orig_win  * ifelse (orig_answer == 'No', n_factor,y_factor)
  
  # if the original answer was yes, then adjust losing probability by the no factor
  new_lose    <- orig_lose * ifelse (orig_answer == 'Yes',n_factor,y_factor)
  
  # alter the answer if the probability order flipped
  new_answer <- as.factor(ifelse( (new_lose > new_win), other_answer, orig_answer))
  
  levels(new_answer) <- levels(orig_factor)
  return(new_answer)
}

         
```

```{r knn_start}
summary(for_knn$Attrition)
```

```{r nb_sanity}
truth <- dplyr::count(employee_data,vars=Attrition)
num_yes <- as.numeric(truth[2,2])
num_no  <- as.numeric(truth[1,2])
num_obs <- num_yes + num_no
true_yes_perc <- round(100*num_yes/num_obs)
true_no_perc  <- round(100*num_no/num_obs)
```

True percentage of "yes" values is `r true_yes_perc`  
True percentage of "no"  values is `r true_no_perc` 

```{r knn_parms}
set.seed(42)
train_frac <- 0.7
# numks <- 50
numks <- 10
# knn_trials <- 250
knn_trials <- 10
positive_value <- 'Yes'
```

```{r do_knn}
# look for the best value of k
#  do 100 trials, and for each trial
#   split data into training and test
#   make classifier and from the confusion matrix,
#   save the accuracy, sensitivity, and specificity
knn_stats = matrix(nrow=numks,ncol=6)
for (i in 1:numks)
{
  acc  = matrix(nrow=knn_trials,ncol=1)
  sens = matrix(nrow=knn_trials,ncol=1)
  spec = matrix(nrow=knn_trials,ncol=1)
  pcyes = matrix(nrow=knn_trials,ncol=1)
  pcno  = matrix(nrow=knn_trials,ncol=1)
  for (j in 1:knn_trials)
  {
    trainIndices = sample(1:dim(for_knn)[1],round(train_frac * dim(for_knn)[1]))
    trainData = for_knn[trainIndices,]
    testData = for_knn[-trainIndices,]
    raw_class = knn(trainData[,cols_for_knn],
                              testData[,cols_for_knn],
                              trainData$Attrition, prob = TRUE, k = i)
    classifications = knn_adjust(raw_class)
    tmp <- data.frame(classifications)
    answers <- count(tmp,vars=classifications)
    num_yes <- as.numeric(answers[2,2])
    num_no  <- as.numeric(answers[1,2])
    num_obs <- num_yes + num_no

    CM = confusionMatrix(table(testData$Attrition,classifications),positive=positive_value)
    acc[j]  = CM$overall[1]
    sens[j] = CM$byClass[1]
    spec[j] = CM$byClass[2]
    pcyes[j] = num_yes / num_obs
    pcno[j]  = num_no / num_obs
  }
  
  # make a matrix with accuracy, sensitivity, and specificity
  knn_stats[i,1] = i
  knn_stats[i,2] = colMeans(acc,na.rm = TRUE)
  knn_stats[i,3] = colMeans(sens,na.rm = TRUE)
  knn_stats[i,4] = colMeans(spec,na.rm = TRUE)
  knn_stats[i,5] = colMeans(pcyes,na.rm = TRUE)
  knn_stats[i,6] = colMeans(pcno,na.rm = TRUE)
}

```

```{r plot_k_stats}
stats_frame <- data.frame(knn_stats)
colnames(stats_frame) <- c("k","accuracy","sensitivity","specificity","Perc_Yes","Perc_No")
for_plot <- reshape2::melt(stats_frame,id.var='k')
for_plot %>% ggplot(aes(x=k,y=value,col=variable)) + 
  geom_line() +
  xlab('k') +
  ylab('percent') +
  ggtitle('Performance of Classifier by K')
```

```{r choose_k}
combined_stats <- stats_frame %>% 
  mutate(sums = accuracy + sensitivity + specificity) 

max_acc_k <- which.max(knn_stats[,2])

chosen_k = 3

chosen_acc  <- round(100*combined_stats$accuracy[chosen_k])
chosen_sens <- round(100*combined_stats$sensitivity[chosen_k])
chosen_spec <- round(100*combined_stats$specificity[chosen_k])
```

max accuracy is at k = `r max_acc_k`.  

we choose `r chosen_k` to go forward in constructing a classifier, since
although performance seems to increase, the number of predicted Yes and No
responses continues to get further from the truth.  The classifier seems to 
be moving more and more in the direction of calling everything a No.

That value of k, on our train and test split, gives and average accuracy of `k chosen_acc`, an average sensitivity of `k chosen_sens` and a specificity of `k chosen_spec`.

```{r knn_eval}
# make a new classifier using all the data
knn_pred <- knn(for_knn[,cols_for_knn],
                for_knn[,cols_for_knn],
                for_knn$Attrition,k=chosen_k)
CM_knn = confusionMatrix(table(as.factor(for_knn$Attrition),knn_pred),positive=positive_value)
knn_acc <- CM_knn$overall[1] # accuracy
knn_sens <- CM_knn$byClass[1] # sensitivity
knn_spec <- CM_knn$byClass[2] # specificity

tmp <- data.frame(knn_pred)
answers <- count(tmp,vars=knn_pred)
num_yes <- as.numeric(answers[2,2])
num_no  <- as.numeric(answers[1,2])
num_obs <- num_yes + num_no
perc_yes <- round(100*num_yes/num_obs)
perc_no  <- round(100*num_no/num_obs)
```

Using the entire dataset,

KNN accuracy turns out to be `r format(100*knn_acc,digits=2)` %.

KNN sensitivity is `r format(100*knn_sens,digits=2)` %.

KNN specificity is `r format(100*knn_spec,digits=2)` %.

```{r knn_pred}
# read in the blind test dataset

blind_raw <- read.csv('CaseStudy2CompSetNoAttrition.csv')
blind_test <- my_transform(blind_raw)
predicted_attrition <- as.factor(knn(for_knn[,cols_for_knn],
                           blind_test[,cols_for_knn],
                           for_knn$Attrition,k=chosen_k))
levels(predicted_attrition) <- c("No","Yes")
summary(predicted_attrition)
for_submission <- cbind(blind_test$ID,as.factor(predicted_attrition))
colnames(for_submission) <- c("ID","Attrition")
write.csv(for_submission,file="Case2PredictionsPalmerAttrition.csv",row.names=FALSE)
```